{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "living-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from collections import OrderedDict\n",
    "from constants import *\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "#unlabeld_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#utils\n",
    "import copy\n",
    "import json\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from statsmodels.stats.inter_rater import cohens_kappa\n",
    "from constants import *\n",
    "\n",
    "#bert labeler\n",
    "from transformers import BertModel, AutoModel\n",
    "\n",
    "#bert_tokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "blond-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_tokenizer\n",
    "class bert_tokenizer:\n",
    "    def get_impressions_from_csv(data):\n",
    "            #df = pd.read_csv(data)'\n",
    "            df = pd.DataFrame(data, columns=['Report Impression'])\n",
    "            imp = df['Report Impression']\n",
    "            imp = imp.str.strip()\n",
    "            imp = imp.replace('\\n',' ', regex=True)\n",
    "            imp = imp.replace('\\s+', ' ', regex=True)\n",
    "            imp = imp.str.strip()\n",
    "            return imp\n",
    "\n",
    "    def tokenize(impressions, tokenizer):\n",
    "            new_impressions = []\n",
    "            print(\"\\nTokenizing report impressions. All reports are cut off at 512 tokens.\")\n",
    "            for i in tqdm(range(impressions.shape[0])):\n",
    "                    tokenized_imp = tokenizer.tokenize(impressions.iloc[i])\n",
    "                    if tokenized_imp: #not an empty report\n",
    "                            res = tokenizer.encode_plus(tokenized_imp)['input_ids']\n",
    "                            if len(res) > 512: #length exceeds maximum size\n",
    "                                    #print(\"report length bigger than 512\")\n",
    "                                    res = res[:511] + [tokenizer.sep_token_id]\n",
    "                            new_impressions.append(res)\n",
    "                    else: #an empty report\n",
    "                            new_impressions.append([tokenizer.cls_token_id, tokenizer.sep_token_id]) \n",
    "            return new_impressions\n",
    "\n",
    "    def load_list(path):\n",
    "            with open(path, 'r') as filehandle:\n",
    "                    impressions = json.load(filehandle)\n",
    "                    return impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "reverse-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert labeler\n",
    "class bert_labeler(nn.Module):\n",
    "    def __init__(self, p=0.1, clinical=False, freeze_embeddings=False, pretrain_path=None):\n",
    "        \"\"\" Init the labeler module\n",
    "        @param p (float): p to use for dropout in the linear heads, 0.1 by default is consistant with \n",
    "                          transformers.BertForSequenceClassification\n",
    "        @param clinical (boolean): True if Bio_Clinical BERT desired, False otherwise. Ignored if\n",
    "                                   pretrain_path is not None\n",
    "        @param freeze_embeddings (boolean): true to freeze bert embeddings during training\n",
    "        @param pretrain_path (string): path to load checkpoint from\n",
    "        \"\"\"\n",
    "        super(bert_labeler, self).__init__()\n",
    "\n",
    "        if pretrain_path is not None:\n",
    "            self.bert = BertModel.from_pretrained(pretrain_path)\n",
    "        elif clinical:\n",
    "            self.bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "        if freeze_embeddings:\n",
    "            for param in self.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        #size of the output of transformer's last layer\n",
    "        hidden_size = self.bert.pooler.dense.in_features\n",
    "        #classes: present, absent, unknown, blank for 12 conditions + support devices\n",
    "        self.linear_heads = nn.ModuleList([nn.Linear(hidden_size, 4, bias=True) for _ in range(13)])\n",
    "        #classes: yes, no for the 'no finding' observation\n",
    "        self.linear_heads.append(nn.Linear(hidden_size, 2, bias=True))\n",
    "\n",
    "    def forward(self, source_padded, attention_mask):\n",
    "        \"\"\" Forward pass of the labeler\n",
    "        @param source_padded (torch.LongTensor): Tensor of word indices with padding, shape (batch_size, max_len)\n",
    "        @param attention_mask (torch.Tensor): Mask to avoid attention on padding tokens, shape (batch_size, max_len)\n",
    "        @returns out (List[torch.Tensor])): A list of size 14 containing tensors. The first 13 have shape \n",
    "                                            (batch_size, 4) and the last has shape (batch_size, 2)  \n",
    "        \"\"\"\n",
    "        #shape (batch_size, max_len, hidden_size)\n",
    "        final_hidden = self.bert(source_padded, attention_mask=attention_mask)[0]\n",
    "        #shape (batch_size, hidden_size)\n",
    "        cls_hidden = final_hidden[:, 0, :].squeeze(dim=1)\n",
    "        cls_hidden = self.dropout(cls_hidden)\n",
    "        out = []\n",
    "        for i in range(14):\n",
    "            out.append(self.linear_heads[i](cls_hidden))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intelligent-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "class utils:\n",
    "    def get_weighted_f1_weights(train_path_or_csv):\n",
    "        \"\"\"Compute weights used to obtain the weighted average of\n",
    "           mention, negation and uncertain f1 scores. \n",
    "        @param train_path_or_csv: A path to the csv file or a dataframe\n",
    "\n",
    "        @return weight_dict (dictionary): maps conditions to a list of weights, the order\n",
    "                                          in the lists is negation, uncertain, positive \n",
    "        \"\"\"\n",
    "        if isinstance(train_path_or_csv, str):\n",
    "            df = pd.read_csv(train_path_or_csv)\n",
    "        else:\n",
    "            df = train_path_or_csv    \n",
    "        df.replace(0, 2, inplace=True)\n",
    "        df.replace(-1, 3, inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        weight_dict = {}\n",
    "        for cond in CONDITIONS:\n",
    "            weights = []\n",
    "            col = df[cond]\n",
    "\n",
    "            mask = col == 2\n",
    "            weights.append(mask.sum())\n",
    "\n",
    "            mask = col == 3\n",
    "            weights.append(mask.sum())\n",
    "\n",
    "            mask = col == 1\n",
    "            weights.append(mask.sum())\n",
    "\n",
    "            if np.sum(weights) > 0:\n",
    "                weights = np.array(weights)/np.sum(weights)\n",
    "            weight_dict[cond] = weights\n",
    "        return weight_dict\n",
    "\n",
    "    def weighted_avg(scores, weights):\n",
    "        \"\"\"Compute weighted average of scores\n",
    "        @param scores(List): the task scores\n",
    "        @param weights (List): corresponding normalized weights\n",
    "\n",
    "        @return (float): the weighted average of task scores\n",
    "        \"\"\"\n",
    "        return np.sum(np.array(scores) * np.array(weights))\n",
    "\n",
    "    def compute_train_weights(train_path):\n",
    "        \"\"\"Compute class weights for rebalancing rare classes\n",
    "        @param train_path (str): A path to the training csv file\n",
    "\n",
    "        @returns weight_arr (torch.Tensor): Tensor of shape (train_set_size), containing\n",
    "                                            the weight assigned to each training example \n",
    "        \"\"\"\n",
    "        df = pd.read_csv(train_path)\n",
    "        cond_weights = {}\n",
    "        for cond in CONDITIONS:\n",
    "            col = df[cond]\n",
    "            val_counts = col.value_counts()\n",
    "            if cond != 'No Finding':\n",
    "                weights = {}\n",
    "                weights['0.0'] = len(df) / val_counts[0]\n",
    "                weights['-1.0'] = len(df) / val_counts[-1]\n",
    "                weights['1.0'] = len(df) / val_counts[1]\n",
    "                weights['nan'] = len(df) / (len(df) - val_counts.sum())\n",
    "            else:\n",
    "                weights = {}\n",
    "                weights['1.0'] = len(df) / val_counts[1]\n",
    "                weights['nan'] = len(df) / (len(df) - val_counts.sum())\n",
    "\n",
    "            cond_weights[cond] = weights\n",
    "\n",
    "        weight_arr = torch.zeros(len(df))\n",
    "        for i in range(len(df)):     #loop over training set\n",
    "            for cond in CONDITIONS:  #loop over all conditions\n",
    "                label = str(df[cond].iloc[i])\n",
    "                weight_arr[i] += cond_weights[cond][label] #add weight for given class' label\n",
    "\n",
    "        return weight_arr\n",
    "\n",
    "    def generate_attention_masks(batch, source_lengths, device):\n",
    "        \"\"\"Generate masks for padded batches to avoid self-attention over pad tokens\n",
    "        @param batch (Tensor): tensor of token indices of shape (batch_size, max_len)\n",
    "                               where max_len is length of longest sequence in the batch\n",
    "        @param source_lengths (List[Int]): List of actual lengths for each of the\n",
    "                               sequences in the batch\n",
    "        @param device (torch.device): device on which data should be\n",
    "\n",
    "        @returns masks (Tensor): Tensor of masks of shape (batch_size, max_len)\n",
    "        \"\"\"\n",
    "        masks = torch.ones(batch.size(0), batch.size(1), dtype=torch.float)\n",
    "        for idx, src_len in enumerate(source_lengths):\n",
    "            masks[idx, src_len:] = 0\n",
    "        return masks.to(device)\n",
    "\n",
    "    def compute_mention_f1(y_true, y_pred):\n",
    "        \"\"\"Compute the mention F1 score as in CheXpert paper\n",
    "        @param y_true (list): List of 14 tensors each of shape (dev_set_size)\n",
    "        @param y_pred (list): Same as y_true but for model predictions\n",
    "\n",
    "        @returns res (list): List of 14 scalars\n",
    "        \"\"\"\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j][y_true[j] == 2] = 1\n",
    "            y_true[j][y_true[j] == 3] = 1\n",
    "            y_pred[j][y_pred[j] == 2] = 1\n",
    "            y_pred[j][y_pred[j] == 3] = 1\n",
    "\n",
    "        res = []\n",
    "        for j in range(len(y_true)): \n",
    "            res.append(f1_score(y_true[j], y_pred[j], pos_label=1))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def compute_blank_f1(y_true, y_pred):\n",
    "        \"\"\"Compute the blank F1 score \n",
    "        @param y_true (list): List of 14 tensors each of shape (dev_set_size)\n",
    "        @param y_pred (list): Same as y_true but for model predictions\n",
    "\n",
    "        @returns res (list): List of 14 scalars                           \n",
    "        \"\"\"\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j][y_true[j] == 2] = 1\n",
    "            y_true[j][y_true[j] == 3] = 1\n",
    "            y_pred[j][y_pred[j] == 2] = 1\n",
    "            y_pred[j][y_pred[j] == 3] = 1\n",
    "\n",
    "        res = []\n",
    "        for j in range(len(y_true)):\n",
    "            res.append(f1_score(y_true[j], y_pred[j], pos_label=0))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def compute_negation_f1(y_true, y_pred):\n",
    "        \"\"\"Compute the negation F1 score as in CheXpert paper\n",
    "        @param y_true (list): List of 14 tensors each of shape (dev_set_size)\n",
    "        @param y_pred (list): Same as y_true but for model predictions   \n",
    "\n",
    "        @returns res (list): List of 14 scalars\n",
    "        \"\"\"\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j][y_true[j] == 3] = 0\n",
    "            y_true[j][y_true[j] == 1] = 0\n",
    "            y_pred[j][y_pred[j] == 3] = 0\n",
    "            y_pred[j][y_pred[j] == 1] = 0\n",
    "\n",
    "        res = []\n",
    "        for j in range(len(y_true)-1):\n",
    "            res.append(f1_score(y_true[j], y_pred[j], pos_label=2))\n",
    "\n",
    "        res.append(0) #No Finding gets score of zero\n",
    "        return res\n",
    "\n",
    "    def compute_positive_f1(y_true, y_pred):\n",
    "        \"\"\"Compute the positive F1 score\n",
    "        @param y_true (list): List of 14 tensors each of shape (dev_set_size)\n",
    "        @param y_pred (list): Same as y_true but for model predictions \n",
    "\n",
    "        @returns res (list): List of 14 scalars\n",
    "        \"\"\"\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j][y_true[j] == 3] = 0\n",
    "            y_true[j][y_true[j] == 2] = 0\n",
    "            y_pred[j][y_pred[j] == 3] = 0\n",
    "            y_pred[j][y_pred[j] == 2] = 0\n",
    "\n",
    "        res = []\n",
    "        for j in range(len(y_true)):\n",
    "            res.append(f1_score(y_true[j], y_pred[j], pos_label=1))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def compute_uncertain_f1(y_true, y_pred):\n",
    "        \"\"\"Compute the negation F1 score as in CheXpert paper\n",
    "        @param y_true (list): List of 14 tensors each of shape (dev_set_size)\n",
    "        @param y_pred (list): Same as y_true but for model predictions\n",
    "\n",
    "        @returns res (list): List of 14 scalars\n",
    "        \"\"\"\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j][y_true[j] == 2] = 0\n",
    "            y_true[j][y_true[j] == 1] = 0\n",
    "            y_pred[j][y_pred[j] == 2] = 0\n",
    "            y_pred[j][y_pred[j] == 1] = 0\n",
    "\n",
    "        res = []\n",
    "        for j in range(len(y_true)-1):\n",
    "            res.append(f1_score(y_true[j], y_pred[j], pos_label=3))\n",
    "\n",
    "        res.append(0) #No Finding gets a score of zero\n",
    "        return res\n",
    "\n",
    "    def evaluate(model, dev_loader, device, f1_weights, return_pred=False):\n",
    "        \"\"\" Function to evaluate the current model weights\n",
    "        @param model (nn.Module): the labeler module \n",
    "        @param dev_loader (torch.utils.data.DataLoader): dataloader for dev set  \n",
    "        @param device (torch.device): device on which data should be\n",
    "        @param f1_weights (dictionary): dictionary mapping conditions to f1\n",
    "                                        task weights\n",
    "        @param return_pred (bool): whether to return predictions or not\n",
    "\n",
    "        @returns res_dict (dictionary): dictionary with keys 'blank', 'mention', 'negation',\n",
    "                               'uncertain', 'positive' and 'weighted', with values \n",
    "                                being lists of length 14 with each element in the \n",
    "                                lists as a scalar. If return_pred is true then a \n",
    "                                tuple is returned with the aforementioned dictionary \n",
    "                                as the first item, a list of predictions as the \n",
    "                                second item, and a list of ground truth as the \n",
    "                                third item\n",
    "        \"\"\"\n",
    "\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        y_pred = [[] for _ in range(len(CONDITIONS))]\n",
    "        y_true = [[] for _ in range(len(CONDITIONS))]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(dev_loader, 0):\n",
    "                batch = data['imp'] #(batch_size, max_len)\n",
    "                batch = batch.to(device)\n",
    "                label = data['label'] #(batch_size, 14)\n",
    "                label = label.permute(1, 0).to(device)\n",
    "                src_len = data['len']\n",
    "                batch_size = batch.shape[0]\n",
    "                attn_mask = generate_attention_masks(batch, src_len, device)\n",
    "\n",
    "                out = model(batch, attn_mask)\n",
    "\n",
    "                for j in range(len(out)):\n",
    "                    out[j] = out[j].to('cpu') #move to cpu for sklearn\n",
    "                    curr_y_pred = out[j].argmax(dim=1) #shape is (batch_size)\n",
    "                    y_pred[j].append(curr_y_pred)\n",
    "                    y_true[j].append(label[j].to('cpu'))\n",
    "\n",
    "                if (i+1) % 200 == 0:\n",
    "                    print('Evaluation batch no: ', i+1)\n",
    "\n",
    "        for j in range(len(y_true)):\n",
    "            y_true[j] = torch.cat(y_true[j], dim=0)\n",
    "            y_pred[j] = torch.cat(y_pred[j], dim=0)\n",
    "\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "        mention_f1 = compute_mention_f1(copy.deepcopy(y_true), copy.deepcopy(y_pred))\n",
    "        negation_f1 = compute_negation_f1(copy.deepcopy(y_true), copy.deepcopy(y_pred))\n",
    "        uncertain_f1 = compute_uncertain_f1(copy.deepcopy(y_true), copy.deepcopy(y_pred))\n",
    "        positive_f1 = compute_positive_f1(copy.deepcopy(y_true), copy.deepcopy(y_pred))\n",
    "        blank_f1 = compute_blank_f1(copy.deepcopy(y_true), copy.deepcopy(y_pred))\n",
    "\n",
    "        weighted = []\n",
    "        kappas = []\n",
    "        for j in range(len(y_pred)):\n",
    "            cond = CONDITIONS[j]\n",
    "            avg = weighted_avg([negation_f1[j], uncertain_f1[j], positive_f1[j]], f1_weights[cond])\n",
    "            weighted.append(avg)\n",
    "\n",
    "            mat = confusion_matrix(y_true[j], y_pred[j])\n",
    "            kappas.append(cohens_kappa(mat, return_results=False))\n",
    "\n",
    "        res_dict = {'mention': mention_f1,\n",
    "                    'blank': blank_f1,\n",
    "                    'negation': negation_f1,\n",
    "                    'uncertain': uncertain_f1,\n",
    "                    'positive': positive_f1,\n",
    "                    'weighted': weighted,\n",
    "                    'kappa': kappas}\n",
    "\n",
    "        if return_pred:\n",
    "            return res_dict, y_pred, y_true\n",
    "        else:\n",
    "            return res_dict\n",
    "\n",
    "    def test(model, checkpoint_path, test_ld, f1_weights):\n",
    "        \"\"\"Evaluate model on test set. \n",
    "        @param model (nn.Module): labeler module\n",
    "        @param checkpoint_path (string): location of saved model checkpoint\n",
    "        @param test_ld (dataloader): dataloader for test set\n",
    "        @param f1_weights (dictionary): maps conditions to f1 task weights\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model) #to utilize multiple GPU's\n",
    "        model = model.to(device)\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        print(\"Doing evaluation on test set\\n\")\n",
    "        metrics = evaluate(model, test_ld, device, f1_weights)\n",
    "        weighted = metrics['weighted']\n",
    "        kappas = metrics['kappa']\n",
    "\n",
    "        for j in range(len(CONDITIONS)):\n",
    "            print('%s kappa: %.3f' % (CONDITIONS[j], kappas[j]))\n",
    "        print('average: %.3f' % np.mean(kappas))\n",
    "\n",
    "        print()\n",
    "        for j in range(len(CONDITIONS)):\n",
    "            print('%s weighted_f1: %.3f' % (CONDITIONS[j], weighted[j]))\n",
    "        print('average of weighted_f1: %.3f' % (np.mean(weighted)))\n",
    "\n",
    "        print()\n",
    "        for j in range(len(CONDITIONS)):\n",
    "            print('%s blank_f1:  %.3f, negation_f1: %.3f, uncertain_f1: %.3f, positive_f1: %.3f' % (CONDITIONS[j],\n",
    "                                                                                                    metrics['blank'][j],\n",
    "                                                                                                    metrics['negation'][j],\n",
    "                                                                                                    metrics['uncertain'][j],\n",
    "                                                                                                    metrics['positive'][j]))\n",
    "\n",
    "        men_macro_avg = np.mean(metrics['mention'])\n",
    "        neg_macro_avg = np.mean(metrics['negation'][:-1]) #No Finding has no negations\n",
    "        unc_macro_avg = np.mean(metrics['uncertain'][:-2]) #No Finding, Support Devices have no uncertain labels in test set\n",
    "        pos_macro_avg = np.mean(metrics['positive'])\n",
    "        blank_macro_avg = np.mean(metrics['blank'])\n",
    "\n",
    "        print(\"blank macro avg: %.3f, negation macro avg: %.3f, uncertain macro avg: %.3f, positive macro avg: %.3f\" % (blank_macro_avg,\n",
    "                                                                                                                        neg_macro_avg,\n",
    "                                                                                                                        unc_macro_avg,\n",
    "                                                                                                                        pos_macro_avg))\n",
    "        print()\n",
    "        for j in range(len(CONDITIONS)):\n",
    "            print('%s mention_f1: %.3f' % (CONDITIONS[j], metrics['mention'][j]))\n",
    "        print('mention macro avg: %.3f' % men_macro_avg)\n",
    "\n",
    "\n",
    "    def label_report_list(checkpoint_path, report_list):\n",
    "        \"\"\" Evaluate model on list of reports.\n",
    "        @param checkpoint_path (string): location of saved model checkpoint\n",
    "        @param report_list (list): list of report impressions (string)\n",
    "        \"\"\"\n",
    "        imp = pd.Series(report_list)\n",
    "        imp = imp.str.strip()\n",
    "        imp = imp.replace('\\n',' ', regex=True)\n",
    "        imp = imp.replace('[0-9]\\.', '', regex=True)\n",
    "        imp = imp.replace('\\s+', ' ', regex=True)\n",
    "        imp = imp.str.strip()\n",
    "\n",
    "        model = bert_labeler()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model) #to utilize multiple GPU's\n",
    "        model = model.to(device)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "\n",
    "        y_pred = []\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        new_imps = tokenize(imp, tokenizer)\n",
    "        with torch.no_grad():\n",
    "            for imp in new_imps:\n",
    "                # run forward prop\n",
    "                imp = torch.LongTensor(imp)\n",
    "                source = imp.view(1, len(imp))\n",
    "\n",
    "                attention = torch.ones(len(imp))\n",
    "                attention = attention.view(1, len(imp))\n",
    "                out = model(source.to(device), attention.to(device))\n",
    "\n",
    "                # get predictions\n",
    "                result = {}\n",
    "                for j in range(len(out)):\n",
    "                    curr_y_pred = out[j].argmax(dim=1) #shape is (1)\n",
    "                    result[CONDITIONS[j]] = CLASS_MAPPING[curr_y_pred.item()]\n",
    "                y_pred.append(result)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "special-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabeld_dataset\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "        \"\"\"The dataset to contain report impressions without any labels.\"\"\"\n",
    "        \n",
    "        def __init__(self, csv_path):\n",
    "                \"\"\" Initialize the dataset object\n",
    "                @param csv_path (string): path to the csv file containing rhe reports. It\n",
    "                                          should have a column named \"Report Impression\"\n",
    "                \"\"\"\n",
    "                tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                impressions = bert_tokenizer.get_impressions_from_csv(csv_path)\n",
    "                self.encoded_imp = bert_tokenizer.tokenize(impressions, tokenizer)\n",
    "\n",
    "        def __len__(self):\n",
    "                \"\"\"Compute the length of the dataset\n",
    "\n",
    "                @return (int): size of the dataframe\n",
    "                \"\"\"\n",
    "                return len(self.encoded_imp)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "                \"\"\" Functionality to index into the dataset\n",
    "                @param idx (int): Integer index into the dataset\n",
    "\n",
    "                @return (dictionary): Has keys 'imp', 'label' and 'len'. The value of 'imp' is\n",
    "                                      a LongTensor of an encoded impression. The value of 'label'\n",
    "                                      is a LongTensor containing the labels and 'the value of\n",
    "                                      'len' is an integer representing the length of imp's value\n",
    "                \"\"\"\n",
    "                if torch.is_tensor(idx):\n",
    "                        idx = idx.tolist()\n",
    "                imp = self.encoded_imp[idx]\n",
    "                imp = torch.LongTensor(imp)\n",
    "                return {\"imp\": imp, \"len\": imp.shape[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "realistic-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Main(label.py)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "PAD_IDX = 0\n",
    "CONDITIONS = ['Enlarged Cardiomediastinum','Cardiomegaly','Lung Opacity','Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis','Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices','No Finding']\n",
    "\n",
    "def collate_fn_no_labels(sample_list):\n",
    "    \"\"\"Custom collate function to pad reports in each batch to the max len,\n",
    "       where the reports have no associated labels\n",
    "    @param sample_list (List): A list of samples. Each sample is a dictionary with\n",
    "                               keys 'imp', 'len' as returned by the __getitem__\n",
    "                               function of ImpressionsDataset\n",
    "\n",
    "    @returns batch (dictionary): A dictionary with keys 'imp' and 'len' but now\n",
    "                                 'imp' is a tensor with padding and batch size as the\n",
    "                                 first dimension. 'len' is a list of the length of \n",
    "                                 each sequence in batch\n",
    "    \"\"\"\n",
    "    tensor_list = [s['imp'] for s in sample_list]\n",
    "    batched_imp = torch.nn.utils.rnn.pad_sequence(tensor_list,\n",
    "                                                  batch_first=True,\n",
    "                                                  padding_value=PAD_IDX)\n",
    "    len_list = [s['len'] for s in sample_list]\n",
    "    batch = {'imp': batched_imp, 'len': len_list}\n",
    "    return batch\n",
    "\n",
    "def load_unlabeled_data(csv_path, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                        shuffle=False):\n",
    "    \"\"\" Create UnlabeledDataset object for the input reports\n",
    "    @param csv_path (string): path to csv file containing reports\n",
    "    @param batch_size (int): the batch size. As per the BERT repository, the max batch size\n",
    "                             that can fit on a TITAN XP is 6 if the max sequence length\n",
    "                             is 512, which is our case. We have 3 TITAN XP's\n",
    "    @param num_workers (int): how many worker processes to use to load data\n",
    "    @param shuffle (bool): whether to shuffle the data or not  \n",
    "    \n",
    "    @returns loader (dataloader): dataloader object for the reports\n",
    "    \"\"\"\n",
    "    collate_fn = collate_fn_no_labels\n",
    "    dset = UnlabeledDataset(csv_path)\n",
    "    loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=shuffle,\n",
    "                                         num_workers=num_workers, collate_fn=collate_fn)\n",
    "    return loader\n",
    "    \n",
    "def label(checkpoint_path, csv_path):\n",
    "    \"\"\"Labels a dataset of reports\n",
    "    @param checkpoint_path (string): location of saved model checkpoint \n",
    "    @param csv_path (string): location of csv with reports\n",
    "\n",
    "    @returns y_pred (List[List[int]]): Labels for each of the 14 conditions, per report  \n",
    "    \"\"\"\n",
    "    ld = load_unlabeled_data(csv_path)\n",
    "    \n",
    "    model = bert_labeler()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.device_count() > 0: #works even if only 1 GPU available\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model) #to utilize multiple GPU's\n",
    "        model = model.to(device)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in checkpoint['model_state_dict'].items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        \n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    y_pred = [[] for _ in range(len(CONDITIONS))]\n",
    "\n",
    "    print(\"\\nBegin report impression labeling. The progress bar counts the # of batches completed:\")\n",
    "    print(\"The batch size is %d\" % BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(ld)):\n",
    "            batch = data['imp'] #(batch_size, max_len)\n",
    "            batch = batch.to(device)\n",
    "            src_len = data['len']\n",
    "            batch_size = batch.shape[0]\n",
    "            attn_mask = utils.generate_attention_masks(batch, src_len, device)\n",
    "\n",
    "            out = model(batch, attn_mask)\n",
    "            for j in range(len(out)):\n",
    "                curr_y_pred = out[j].argmax(dim=1) #shape is (batch_size)\n",
    "                print(len(out), j, curr_y_pred)\n",
    "                y_pred[j].append(curr_y_pred)\n",
    "\n",
    "        for j in range(len(y_pred)):\n",
    "            y_pred[j] = torch.cat(y_pred[j], dim=0)\n",
    "             \n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    y_pred = [t.tolist() for t in y_pred]\n",
    "    return y_pred\n",
    "\n",
    "def save_preds(y_pred, csv_path, out_path):\n",
    "    \"\"\"Save predictions as out_path/labeled_reports.csv \n",
    "    @param y_pred (List[List[int]]): list of predictions for each report\n",
    "    @param csv_path (string): path to csv containing reports\n",
    "    @param out_path (string): path to output directory\n",
    "    \"\"\"\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.T\n",
    "    \n",
    "    df = pd.DataFrame(y_pred, columns=CONDITIONS)\n",
    "    reports = pd.read_csv(csv_path)['Report Impression']\n",
    "\n",
    "    df['Report Impression'] = reports.tolist()\n",
    "    new_cols = ['Report Impression'] + CONDITIONS\n",
    "    df = df[new_cols]\n",
    "\n",
    "    df.replace(0, np.nan, inplace=True) #blank class is NaN\n",
    "    df.replace(3, -1, inplace=True)     #uncertain class is -1\n",
    "    df.replace(2, 0, inplace=True)      #negative class is 0 \n",
    "    \n",
    "    df.to_csv(os.path.join(out_path, 'labeled_reports.csv'), index=False)\n",
    "    \n",
    "\n",
    "    \n",
    "def set_flags(**args):\n",
    "            flag = SimpleNamespace(**args)\n",
    "            return flag\n",
    "        \n",
    "\n",
    "def run(report, ckpt_path):\n",
    "    data = [report]\n",
    "    ckpt = ckpt_path\n",
    "    \n",
    "    y_pred = label(ckpt, data)\n",
    "    \n",
    "    report = pd.DataFrame(data, columns=['Report Impression'])\n",
    "    result = np.asarray(y_pred).T\n",
    "    df = pd.DataFrame(result, columns=CONDITIONS)\n",
    "    df = report.join(df)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acknowledged-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 996.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing report impressions. All reports are cut off at 512 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin report impression labeling. The progress bar counts the # of batches completed:\n",
      "The batch size is 32\n",
      "14 0 tensor([0])\n",
      "14 1 tensor([0])\n",
      "14 2 tensor([0])\n",
      "14 3 tensor([0])\n",
      "14 4 tensor([0])\n",
      "14 5 tensor([0])\n",
      "14 6 tensor([0])\n",
      "14 7 tensor([0])\n",
      "14 8 tensor([0])\n",
      "14 9 tensor([0])\n",
      "14 10 tensor([0])\n",
      "14 11 tensor([0])\n",
      "14 12 tensor([0])\n",
      "14 13 tensor([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Impression</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_reports</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Report Impression  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0    sample_reports                           0             0             0   \n",
       "\n",
       "   Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  Pneumothorax  \\\n",
       "0            0      0              0          0            0             0   \n",
       "\n",
       "   Pleural Effusion  Pleural Other  Fracture  Support Devices  No Finding  \n",
       "0                 0              0         0                0           1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run('sample_reports', 'ckpt/chexbert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-remark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chexbert] *",
   "language": "python",
   "name": "conda-env-chexbert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
